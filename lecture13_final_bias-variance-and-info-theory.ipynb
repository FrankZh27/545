{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\X}{\\mathcal{X}}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d5797058a9b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt;\n",
    "import seaborn as sns\n",
    "import pylab as pl\n",
    "from matplotlib.pylab import cm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# scientific\n",
    "import numpy as np;\n",
    "\n",
    "# ipython\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 545:  Machine Learning\n",
    "## Lecture 13:  Information Theory and Exponential Families\n",
    "* Instructor:  Dave Daniszewski, using material from **Jacob Abernethy**\n",
    "* Date:  February 16, 2017\n",
    "\n",
    "*Lecture Exposition Credit:*  Benjamin Bray & Saket Dewangan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review of Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias and Variance Formulae\n",
    "\n",
    "* Recall $y = f + \\epsilon$, where $\\epsilon$ is some $0$-mean noise with var. $\\sigma^2$\n",
    "* Alg receives dataset $S$ and outputs $\\hat f$, prediction of $y$. The error is:\n",
    "\n",
    "$$\\mathbb{E}[(y - \\hat{f})^2] = \\underbrace{{\\sigma^2}}_\\text{irreducible error} + \\underbrace{{\\text{Var}[\\hat{f}]}}_\\text{Variance} + \\underbrace{{\\mathbb{E}[f - \\mathbb{E}_S[\\hat{f}]]}^2}_{\\text{Bias}^2}$$\n",
    "\n",
    "* Break error into two terms relating to $\\mathbb{E}_{S}[\\hat f]$ the \"average\" estimate over random datasets $S$.\n",
    "    * Bias of an estim.: $\\text{Bias}(\\hat{f}) = (\\mathbb{E}_S[\\hat{f}] - f)$\n",
    "    * Variance of estim.: $\\text{Var}(\\hat{f}) = \\mathbb{E}[(\\hat{f} - \\mathbb{E}_S[\\hat{f}])^2]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An example to explain Bias/Variance and illustrate the tradeoff \n",
    "\n",
    "- Consider estimating a sinusoidal function. \n",
    "\n",
    "(Example that follows is inspired by Yaser Abu-Mostafa's CS 156 Lecture titled \"Bias-Variance Tradeoff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "RANGEXS = np.linspace(0., 2., 300)\n",
    "TRUEYS = np.sin(np.pi * RANGEXS)\n",
    "\n",
    "def plot_fit(x, y, p, show,color='k'):\n",
    "    xfit = RANGEXS\n",
    "    yfit = np.polyval(p, xfit)\n",
    "    if show:\n",
    "        axes = pl.gca()\n",
    "        axes.set_xlim([min(RANGEXS),max(RANGEXS)])\n",
    "        axes.set_ylim([-2.5,2.5])\n",
    "        pl.scatter(x, y, facecolors='none', edgecolors=color)\n",
    "        pl.plot(xfit, yfit,color=color)\n",
    "        pl.hold('on')\n",
    "        pl.xlabel('x')\n",
    "        pl.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def calc_errors(p):\n",
    "    x = RANGEXS\n",
    "    errs = []\n",
    "    for i in x:\n",
    "        errs.append(abs(np.polyval(p, i) - np.sin(np.pi * i)) ** 2)\n",
    "    return errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bias_variance(poly_coeffs, input_values_x, true_values_y):\n",
    "    # poly_coeffs: a list of polynomial coefficient vectors\n",
    "    # input_values_x: the range of xvals we will see\n",
    "    # true_values_y: the true labels/targes for y\n",
    "\n",
    "    # First we calculate the mean polynomial, and compute the predictions for this mean poly\n",
    "    mean_coeffs = np.mean(poly_coeffs, axis=0)\n",
    "    mean_predicted_poly = np.poly1d(mean_coeffs)\n",
    "    mean_predictions_y = np.polyval(mean_predicted_poly, input_values_x)\n",
    "    \n",
    "    # Then we calculate the error of this mean poly\n",
    "    bias_errors_across_x = (mean_predictions_y - true_values_y) ** 2\n",
    "    \n",
    "    # To consider the variance errors, we need to look at every output of the coefficients\n",
    "    variance_errors = []\n",
    "    for coeff in poly_coeffs:\n",
    "        predicted_poly = np.poly1d(coeff)\n",
    "        predictions_y = np.polyval(predicted_poly, input_values_x)\n",
    "        # Variance error is the average squared error between the predicted values of y\n",
    "        # and the *average* predicted value of y\n",
    "        variance_error = (mean_predictions_y - predictions_y)**2\n",
    "        variance_errors.append(variance_error)\n",
    "\n",
    "    variance_errors_across_x = np.mean(np.array(variance_errors),axis=0)\n",
    "    \n",
    "    return bias_errors_across_x, variance_errors_across_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def polyfit_sin(degree=0, iterations=100, num_points=5, show=True):\n",
    "    total = 0\n",
    "    l = []\n",
    "    coeffs = []\n",
    "    errs = [0] * len(RANGEXS)\n",
    "    colors=cm.rainbow(np.linspace(0,1,iterations))\n",
    "    for i in range(iterations):\n",
    "        np.random.seed()\n",
    "        x = np.random.choice(RANGEXS,size=num_points) # Pick random points from the sinusoid\n",
    "        y = np.sin(np.pi * x)\n",
    "        p = np.polyfit(x, y, degree)  \n",
    "        y_poly = [np.polyval(p, x_i) for x_i in x]  \n",
    "        plot_fit(x, y, p, show,color=colors[i])\n",
    "        total += sum(abs(y_poly - y) ** 2) # calculate Squared Error (Squared Error) \n",
    "        coeffs.append(p)\n",
    "        errs = np.add(calc_errors(p), errs)\n",
    "    return total / iterations, errs / iterations, np.mean(coeffs, axis = 0), coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_bias_and_variance(biases,variances,range_xs,true_ys,mean_predicted_ys):\n",
    "    pl.plot(range_xs, mean_predicted_ys, c='k')\n",
    "    axes = pl.gca()\n",
    "    axes.set_xlim([min(range_xs),max(range_xs)])\n",
    "    axes.set_ylim([-3,3])\n",
    "    pl.hold('on')\n",
    "    pl.plot(range_xs, true_ys,c='b')\n",
    "    pl.errorbar(range_xs, mean_predicted_ys, yerr = biases, c='y', ls=\"None\", zorder=0,alpha=1)\n",
    "    pl.errorbar(range_xs, mean_predicted_ys, yerr = variances, c='r', ls=\"None\", zorder=0,alpha=0.1)\n",
    "    pl.xlabel('x')\n",
    "    pl.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's return to fitting polynomials\n",
    "\n",
    "* Here we generate some samples $x,y$, with $y = \\sin(2\\pi x)$\n",
    "* We then fit a degree-$0$ polynomial (i.e. a constant function) to the samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# polyfit_sin() generates 5 samples of the form (x,y) where y=sin(2*pi*x)\n",
    "# then it tries to fit a degree=0 polynomial (i.e. a constant func.) to the data\n",
    "# Ignore return values for now, we will return to these later\n",
    "_, _, _, _ = polyfit_sin(degree=0, iterations=1, num_points=5, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We can do this over many datasets\n",
    "\n",
    "* Let's sample a number of datasets\n",
    "* How does the fitted polynomial change for different datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate two points of sin(pi * x) with a constant 5 times\n",
    "_, _, _, _ = polyfit_sin(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What about over lots more datasets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate two points of sin(pi * x) with a constant 100 times\n",
    "_, _, _, _ = polyfit_sin(0, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "MSE, errs, mean_coeffs, coeffs_list = polyfit_sin(0, 100,num_points = 3,show=False)\n",
    "biases, variances = calculate_bias_variance(coeffs_list,RANGEXS,TRUEYS)\n",
    "plot_bias_and_variance(biases,variances,RANGEXS,TRUEYS,np.polyval(np.poly1d(mean_coeffs), RANGEXS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Decomposition: $\\mathbb{E}[(y - \\hat{f})^2] = \\underbrace{{\\sigma^2}}_\\text{irreducible error} + \\underbrace{{\\text{Var}[\\hat{f}]}}_\\text{Variance} + \\underbrace{{\\mathbb{E}[f - \\mathbb{E}_S[\\hat{f}]]}^2}_{\\text{Bias}^2}$\n",
    "* Blue curve: true $f$\n",
    "* Black curve: $\\hat f$, average predicted values of $y$\n",
    "* Yellow is error due to **Bias**, Red/Pink is error due to **Variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias vs. Variance\n",
    "* We can calculate how much error we suffered due to bias and due to variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "poly_degree = 0\n",
    "results_list = []\n",
    "MSE, errs, mean_coeffs, coeffs_list = polyfit_sin(\n",
    "    poly_degree, 500,num_points = 5,show=False)\n",
    "biases, variances = calculate_bias_variance(coeffs_list,RANGEXS,TRUEYS)\n",
    "sns.barplot(x='type', y='error',hue='poly_degree', data=pd.DataFrame([\n",
    "    {'error':np.mean(biases), 'type':'bias','poly_degree':0},\n",
    "    {'error':np.mean(variances), 'type':'variance','poly_degree':0}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's now fit degree=3 polynomials\n",
    "\n",
    "* Let's sample a dataset of 5 points and fit a cubic poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "MSE, _, _, _ = polyfit_sin(degree=3, iterations=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's now fit degree=3 polynomials\n",
    "\n",
    "* What does this look like over 5 different datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = polyfit_sin(degree=3,iterations=5,num_points=5,show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's now fit degree=3 polynomials\n",
    "\n",
    "* What does this look like over 50 different datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate two points of sin(pi * x) with a line 50 times\n",
    "_, _, _, _ = polyfit_sin(degree=3, iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "MSE, errs, mean_coeffs, coeffs_list = polyfit_sin(3,500,show=False)\n",
    "biases, variances = calculate_bias_variance(coeffs_list,RANGEXS,TRUEYS)\n",
    "plot_bias_and_variance(biases,variances,RANGEXS,TRUEYS,np.polyval(np.poly1d(mean_coeffs), RANGEXS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\\mathbb{E}[(y - \\hat{f})^2] = \\underbrace{{\\sigma^2}}_\\text{irreducible error} + \\underbrace{{\\text{Var}[\\hat{f}]}}_\\text{Variance} + \\underbrace{{\\mathbb{E}[f - \\mathbb{E}_S[\\hat{f}]]}^2}_{\\text{Bias}^2}$$\n",
    "* Blue curve: true $f$\n",
    "* Black curve: $\\mathbb{E}[\\hat f]$, average prediction of $y$\n",
    "* Yellow is error due to **Bias**, Red/Pink is error due to **Variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias and Variance for different degree sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for poly_degree in [0,1,3]:\n",
    "    MSE, errs, mean_coeffs, coeffs_list = polyfit_sin(poly_degree,500,num_points=5,show=False)\n",
    "    biases, variances = calculate_bias_variance(coeffs_list,RANGEXS,TRUEYS)\n",
    "    results_list.append({'error':np.mean(biases),\n",
    "                         'type':'bias', 'poly_degree':poly_degree})\n",
    "    results_list.append({'error':np.mean(variances),\n",
    "                         'type':'variance', 'poly_degree':poly_degree})\n",
    "sns.barplot(x='type', y='error',hue='poly_degree',data=pd.DataFrame(results_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* High degree polys have lower bias but much greater variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Info Theory + Exponential Familes -- References\n",
    "\n",
    "Information Theory:\n",
    "- **[Shannon 1951]** Shannon, Claude E.. [*The Mathematical Theory of Communication*](http://worrydream.com/refs/Shannon%20-%20A%20Mathematical%20Theory%20of%20Communication.pdf).  1951.\n",
    "- **[Pierce 1980]** Pierce, John R..  [*An Introduction to Information Theory:  Symbols, Signals, and Noise*](http://www.amazon.com/An-Introduction-Information-Theory-Mathematics/dp/0486240614).  1980.\n",
    "- **[Stone 2015]** Stone, James V..  [*Information Theory:  A Tutorial Introduction*](http://jim-stone.staff.shef.ac.uk/BookInfoTheory/InfoTheoryBookMain.html).  2015.\n",
    "\n",
    "Exponential Families:\n",
    "- **[MLAPP]** Murphy, Kevin. [*Machine Learning:  A Probabilistic Perspective*](https://mitpress.mit.edu/books/machine-learning-0).  2012.\n",
    "- **[Hero 2008]** Hero, Alfred O..  [*Statistical Methods for Signal Processing*](http://web.eecs.umich.edu/~hero/Preprints/main_564_08_new.pdf).  2008.\n",
    "- **[Blei 2011]** Blei, David. [*Notes on Exponential Families*](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/exponential-families.pdf).  2011.\n",
    "- **[Wainwright & Jordan 2008]** Wainwright, Martin J. and Michael I. Jordan.  [*Graphical Models, Exponential Families, and Variational Inference*](https://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf).  2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "This lecture, we introduce some important background for **Probabilistic Graphical Models**.\n",
    "\n",
    "* Information Theory\n",
    "    - Information, Entropy, and Encoding\n",
    "    - Relative Entropy, Mutual Information & Collocations\n",
    "    - Maximum Entropy Distributions\n",
    "* Exponential Family\n",
    "    - Mean and Natural Parameterizations\n",
    "    - Conjugate Priors & Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Information Theory\n",
    "\n",
    "> Uses material from **[MLAPP]** §2.8, **[Pierce 1980]**, **[Stone 2015]**, and **[Shannon 1951]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Information Theory\n",
    "\n",
    "Information theory is concerned with\n",
    "- **Compression:**  Representing data in a compact fashion\n",
    "- **Error Correction:**  Transmitting and storing data in a way that is robust to errors\n",
    "\n",
    "In machine learning, information-theoretic quantities are useful for\n",
    "- manipulating probability distributions\n",
    "- interpreting statistical learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Information?\n",
    "\n",
    "Can we measure the amount of **information** we gain from an observation?\n",
    "- Information is measured in *bits* ( don't confuse with *binary digits*, $0110001\\dots$ )\n",
    "- Intuitively, observing a fair coin flip should give 1 bit of information\n",
    "- Observing two fair coins should give 2 bits, and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Information:  Definition\n",
    "\n",
    "The **information content** of an event $E$ with probability $p$ is\n",
    "$$\n",
    "I(E) = I(p) = - \\log_2 p = \\log_2 \\frac{1}{p} \\geq 0\n",
    "$$\n",
    "\n",
    "- Information theory is about *probabilities* and *distributions*\n",
    "- The \"meaning\" of events doesn't matter.\n",
    "- Using bases other than 2 yields different units (Hartleys, nats, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Fair Coin\n",
    "\n",
    "**One Coin:**  If $P(Heads)=0.5$ and we observe heads, then\n",
    "$$\n",
    "I(Heads) = - \\log_2 P(Heads) = 1 \\;\\mathrm{bit}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Two Coins:** If we observe two heads in a row, \n",
    "$$\n",
    "\\begin{align}\n",
    "I(Heads,Heads)\n",
    "&= -\\log_2 P(Heads, Heads) \\\\\n",
    "&= -\\log_2 P(Heads)P(Heads) \\\\\n",
    "&= -\\log_2 P(Heads) - \\log_2 P(Heads) = 2 \\;\\mathrm{bits}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Unfair Coin\n",
    "\n",
    "Suppose the coin has two heads, so $P(H)=1$.  Then,\n",
    "$$\n",
    "I(Heads) = - \\log_2 1 = 0\n",
    "$$\n",
    "\n",
    "If we know the coin is unfair, we gain no information by observing heads!\n",
    "- Information is a measure of how **surprised** we are by an outcome.\n",
    "- Observing heads when $P(H)=0$ yields *infinite* information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy:  Definition\n",
    "\n",
    "The **entropy** of a discrete random variable $X$ with distribution $p$ is\n",
    "$$\n",
    "H[X] = H[p] = E[I(p(X))] = - \\sum_{x \\in X} p(x) \\log p(x)\n",
    "$$\n",
    "\n",
    "Entropy is the expected information received when we sample from $X$.\n",
    "- How *surprised* are we, on average?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy:  Coin Flip\n",
    "\n",
    "If $X$ is binary, $H[X] = -[ p \\log p + (1-p) \\log (1-p) ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "p = np.linspace(0.01,0.99,100);\n",
    "plt.plot(p, -(p * np.log(p) + (1-p)*np.log(1-p)) / np.log(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy & Surprisal\n",
    "\n",
    "Entropy is highest when $X$ is close to uniform.\n",
    "- Large entropy $\\iff$ high uncertainty, more information from each new observation\n",
    "- Low entropy $\\iff$ more knowledge about possible outcomes\n",
    "\n",
    "The farther from uniform $X$ is, the lower the entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Break Time!\n",
    "\n",
    "![](images/silly_cat.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Principle\n",
    "\n",
    "Suppose we sample data from an unknown distribution $p$, and\n",
    "- we collect statistics (mean, variance, etc.) from the data\n",
    "- we want an *objective* or unbiased estimate of $p$\n",
    "\n",
    "The **Maximum Entropy Principle** states that:\n",
    "\n",
    "> We should choose $p$ to have maximum entropy $H[p]$ among all distributions satisfying our constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy:  Examples\n",
    "\n",
    "Some examples of maximum entropy distributions:\n",
    "\n",
    "<table>\n",
    "<thead><th>Constraints</th><th>Maximum Entropy Distribution</th></thead>\n",
    "<tbody>\n",
    "    <tr><td>Min $a$, Max $b$</td><td>Uniform $U[a,b]$</td></tr>\n",
    "    <tr><td>Mean $\\mu$, Support $(0,+\\infty)$</td><td>Exponential $Exp(\\mu)$</td></tr>\n",
    "    <tr><td>Mean $\\mu$, Variance $\\sigma^2$</td><td>Gaussian $\\mathcal{N}(\\mu, \\sigma^2)$</td></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Later, **Exponential Family Distributions** will generalize this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Communication Channels\n",
    "\n",
    "For some intuition, consider a **communication channel**:\n",
    "1. The **source** generates messages.\n",
    "2. An **encoder** converts the message to a **signal** for transmission.\n",
    "3. Signals are transmitted along a **channel**, possibly under the influence of **noise**.\n",
    "4. A **decoder** attempts to reconstruct the original message from the transmitted signal.\n",
    "5. The **destination** is the intended recipient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"images/shannon_comm_channel.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoding\n",
    "\n",
    "Suppose we draw messages from a distribution $p$.\n",
    "- Certain messages may be more likely than others.\n",
    "- For example, the letter $e$ is most frequent in English\n",
    "\n",
    "An **efficient** encoding minimizes the average message length,\n",
    "- assign *short* codewords to common messages\n",
    "- and *longer* codewords to rare messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Interesting side note on Morse Code\n",
    "At the time, newspaper printers had tiny metal copies of each letter, used for printing.  A researcher apparently reasoned that they would have only as many copies of each letter as necessary to print a page, so he counted the number of copies of each letter they had and used that to estimate English letter frequencies.\n",
    "\n",
    "[Wikipedia reference](https://en.wikipedia.org/wiki/Morse_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoding:  Morse Code\n",
    "\n",
    "This is precisely how **Morse Code** works!\n",
    "> Approximates **Huffman Coding**, which gives optimal binary codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"images/morse-code.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Source Coding Theorem\n",
    "\n",
    "Claude Shannon proved that for discrete noiseless channels:\n",
    "\n",
    "> It is impossible to encode messages drawn from a distribution $p$ with fewer than $H[p]$ bits, on average.\n",
    "\n",
    "Here, *bits* refers to *binary digits*, i.e. encoding messages in binary.\n",
    "> $H[p]$ measures the optimal code length, in bits, for messages drawn from $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Entropy & Relative Entropy\n",
    "\n",
    "Consider different distributions $p$ and $q$\n",
    "- What if we use a code optimal for $q$ to encode messages from $p$?\n",
    "\n",
    "For example, suppose our encoding scheme is optimal for German text.\n",
    "- What if we send English messages instead?\n",
    "- Certainly, there will be some waste due to different letter frequencies, umlauts, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Entropy\n",
    "\n",
    "The **cross entropy** measures the average number of bits needed to encode messages drawn from $p$ when we use a code optimal for $q$:\n",
    "$$\n",
    "H(p,q) = -\\sum_{x \\in \\X} p(x) \\log q(x)\n",
    "= - E_p[\\log q(x)]\n",
    "$$\n",
    "\n",
    "Intuitively, $H(p,q) \\geq H(p)$.  The **relative entropy** is the difference $H(p,q) - H(p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relative Entropy:  Definition\n",
    "\n",
    "The **relative entropy** or **Kullback-Leibler divergence** of $q$ from $p$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(p || q)\n",
    "&= \\sum_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)} \\\\\n",
    "&= H(p,q) - H(p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Measures the number of *extra* bits needed to encode messages from $p$ if we use a code optimal for $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mutual Information:  Definition\n",
    "\n",
    "The **mutual information** between discrete variables $X$ and $Y$ is\n",
    "$$\\begin{align}\n",
    "I(X; Y)\n",
    "&= \\sum_{y\\in Y} \\sum_{x \\in X} p(x,y) \\log\\frac{p(x,y)}{p(x)p(y)} \\\\\n",
    "&= D_{KL}( p(x,y) || p(x)p(y) )\n",
    "\\end{align}$$\n",
    "\n",
    "- If $X$ and $Y$ are independent, $p(x,y)=p(x)p(y)$\n",
    "- So, $I(X;Y)$ measures how *dependent* $X$ and $Y$ are!\n",
    "- Related to correlation $\\rho(X,Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Collocations & PMI\n",
    "\n",
    "A **collocation** is a sequence of words that co-occur more often than expected by chance.\n",
    "- fixed expression familiar to native speakers (hard to translate)\n",
    "- meaning of the whole is more than the sum of its parts\n",
    "\n",
    "> See [these slides](https://www.eecis.udel.edu/~trnka/CISC889-11S/lectures/philip-pmi.pdf) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Collocation & PMI\n",
    "\n",
    "Substituting a synonym sounds unnatural:\n",
    "- \"fast food\" vs. \"quick food\"\n",
    "- \"Great Britain\" vs. \"Good Britain\"\n",
    "- \"warm greetings\" vs \"hot greetings\"\n",
    "\n",
    "> How can we find collocations in a corpus of text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Collocations & PMI\n",
    "\n",
    "The **pointwise mutual information** between words $x$ and $y$ is\n",
    "$$\n",
    "\\mathrm{pmi}(x;y) = \\log \\frac{p(x,y)}{p(x)p(y)}\n",
    "$$\n",
    "\n",
    "- $p(x)p(y)$ is how frequently we **expect** $x$ and $y$ to co-occur, if they do so independently.\n",
    "- $p(x,y)$ measures how frequently $x$ and $y$ **actually** occur together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Collocations & PMI\n",
    "\n",
    "**Idea:**  Rank word pairs by $\\mathrm{pmi}(x,y)$ to find collocations!\n",
    "- $\\mathrm{pmi}(x,y)$ is large if $x$ and $y$ co-occur more frequently together than expected\n",
    "\n",
    "**Code:** Let's try it on the novel *Crime and Punishment*!\n",
    "- Pre-computed unigram and bigram counts are found in the `collocations/data` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Collocations & PMI\n",
    "\n",
    "Here we read in the precomputed data.  See the notebook in the `collocations` folder for a full implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import csv, math;\n",
    "\n",
    "# file paths\n",
    "unigram_path = \"collocations/data/crime-and-punishment.txt.unigrams\";\n",
    "bigram_path = \"collocations/data/crime-and-punishment.txt.bigrams\";\n",
    "\n",
    "# read unigrams into dict\n",
    "with open(unigram_path) as f:\n",
    "    reader = csv.reader(f);\n",
    "    unigrams = { row[0] : int(row[1]) for row in csv.reader(f)};\n",
    "    \n",
    "# read bigrams into dict\n",
    "with open(bigram_path) as f:\n",
    "    reader = csv.reader(f);\n",
    "    bigrams = { (row[0],row[1]) : int(row[2]) for row in csv.reader(f)};\n",
    "\n",
    "# pretty print table\n",
    "class PrettyTable(object):\n",
    "        def __init__(self, data, head1, head2, floats=False):\n",
    "            table = \"<table>\"\n",
    "            table += \"<thead><th>%s</th><th>%s</th></thead>\\n\" % (head1,head2);\n",
    "            table += \"<tbody>\\n\"\n",
    "            for bigram,count in data:\n",
    "                if floats: count = \"%0.2f\" % count;\n",
    "                else: count = \"%d\" % count;\n",
    "\n",
    "                table += \"<tr>\"\n",
    "                table += \"<td>%s %s</td>\" % bigram;\n",
    "                table += \"<td>%s</td>\" % count;\n",
    "                table += \"</tr>\\n\";\n",
    "\n",
    "            table += \"</tbody></table>\"\n",
    "            self.table = table;\n",
    "        \n",
    "        def _repr_html_(self):\n",
    "            return self.table;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Collocations & PMI\n",
    "\n",
    "The following code sorts bigrams by pointwise mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# compute pmi\n",
    "pmi_bigrams = [];\n",
    "\n",
    "for w1,w2 in bigrams:\n",
    "    # compute pmi\n",
    "    actual = bigrams[(w1,w2)];\n",
    "    expected = unigrams[w1] * unigrams[w2];\n",
    "    pmi = math.log( actual / expected );\n",
    "    # filter out infrequent bigrams\n",
    "    if actual < 15: continue;\n",
    "    pmi_bigrams.append( ((w1, w2), pmi) );\n",
    "\n",
    "# sort pmi\n",
    "pmi_sorted = sorted(pmi_bigrams, key=lambda x: x[1], reverse=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Collocations & PMI\n",
    "\n",
    "Here are the most frequent bigrams--these aren't collocations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bigrams_sorted = sorted(bigrams.items(), key=lambda x: x[1], reverse=True);\n",
    "PrettyTable(bigrams_sorted[:10], \"Bigram\", \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Collocations & PMI\n",
    "\n",
    "Sorting bigrams by PMI, we first get names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "PrettyTable(pmi_sorted[1:10], \"Collocation\", \"PMI\", floats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Collocations & PMI\n",
    "\n",
    "...then more interesting collocations!  This is much more useful than sorting by frequency alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "PrettyTable(pmi_sorted[12:20], \"Collocation\", \"PMI\", floats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Feature Selection\n",
    "\n",
    "Mutual information can also be used for **feature selection**.\n",
    "- In classification, features that *depend* most on the class label $C$ are useful\n",
    "- So, choose features $X_k$ such that $I(X_k ; C)$ is large\n",
    "- This helps to avoid *overfitting* by ignoring irrelevant features!\n",
    "\n",
    "> See **[MLAPP]** §3.5.4 for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exponential Families\n",
    "\n",
    "> Uses material from **[MLAPP]** §9.2 and **[Hero 2008]** §3.5, §4.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family: Introduction\n",
    "\n",
    "We have seen many distributions.\n",
    "* Bernoulli\n",
    "* Gaussian\n",
    "* Exponential\n",
    "* Gamma \n",
    "    \n",
    "Many of these belong to a more general class called the **exponential family**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  Introduction\n",
    "\n",
    "Why do we care?\n",
    "* only family of distributions with finite-dimensional **sufficient statistics**\n",
    "* only family of distributions for which **conjugate priors** exist\n",
    "* makes the least set of assumptions subject to some user-chosen constraints (**Maximum Entropy**)\n",
    "* core of generalized linear models and **variational inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sufficient Statistics\n",
    "\n",
    "**Recall:** A **statistic** $T(\\D)$ is a function of the observed data $\\D$.\n",
    "- Mean, $T(x_1, \\dots, x_n) = \\frac{1}{n}\\sum_{k=1}^n x_k$\n",
    "- Variance, maximum, mode, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sufficient Statistics:  Definition\n",
    "\n",
    "Suppose we have a model $P$ with parameters $\\theta$.  Then,\n",
    "\n",
    "> A statistic $T(\\D)$ is **sufficient** for $\\theta$ if no other statistic calculated from the same sample provides any additional information about the parameter.\n",
    "\n",
    "That is, if $T(\\D_1) = T(\\D_2)$, our estimate of $\\theta$ given $\\D_1$ or $\\D_2$ will be the same.\n",
    "- Mathematically, $P(\\theta | T(\\D), \\D) = P(\\theta | T(\\D))$ independently of $\\D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sufficient Statistics:  Example\n",
    "\n",
    "Suppose $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and we observe $\\mathcal{D} = (x_1, \\dots, x_n)$.  Let\n",
    "- $\\hat\\mu$ be the sample mean\n",
    "- $\\hat{\\sigma}^2$ be the sample variance\n",
    "\n",
    "Then $T(\\mathcal{D}) = (\\hat\\mu, \\hat{\\sigma}^2)$ is sufficient for $\\theta=(\\mu, \\sigma^2)$.\n",
    "- Two samples $\\D_1$ and $\\D_2$ with the same mean and variance give the same estimate of $\\theta$\n",
    "\n",
    "<span style=\"color:gray\">(we are sweeping some details under the rug)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  Definition\n",
    "\n",
    "$p(x | \\theta)$ has **exponential family form** if:\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x | \\theta)\n",
    "&= \\frac{1}{Z(\\theta)} h(x) \\exp\\left[ \\eta(\\theta)^T \\phi(x) \\right] \\\\\n",
    "&= h(x) \\exp\\left[ \\eta(\\theta)^T \\phi(x) - A(\\theta) \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $Z(\\theta)$ is the **partition function** for normalization\n",
    "- $A(\\theta) = \\log Z(\\theta)$ is the **log partition function**\n",
    "- $\\phi(x) \\in \\R^d$ is a vector of **sufficient statistics**\n",
    "- $\\eta(\\theta)$ maps $\\theta$ to a set of **natural parameters**\n",
    "- $h(x)$ is a scaling constant, usually $h(x)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Bernoulli\n",
    "\n",
    "The Bernoulli distribution can be written as\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{Ber}(x | \\mu)\n",
    "&= \\mu^x (1-\\mu)^{1-x} \\\\\n",
    "&= \\exp\\left[ x \\log \\mu + (1-x) \\log (1-\\mu) \\right] \\\\\n",
    "&= \\exp\\left[ \\eta(\\mu)^T \\phi(x) \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\eta(\\mu) = (\\log\\mu, \\log(1-\\mu))$ and $\\phi(x) = (x, 1-x)$\n",
    "- There is a linear dependence between features $\\phi(x)$\n",
    "- This representation is **overcomplete**\n",
    "- $\\eta$ is not uniquely determined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Bernoulli\n",
    "\n",
    "Instead, we can find a **minimal** parameterization:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{Ber}(x | \\mu) \n",
    "&= (1-\\mu) \\exp\\left[ x \\log\\frac{\\mu}{1-\\mu} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This gives **natural parameters** $\\eta = \\log \\frac{\\mu}{1-\\mu}$.\n",
    "- Now, $\\eta$ is unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Examples\n",
    "\n",
    "Exponential Family Distributions:\n",
    "- Multivariate normal\n",
    "- Exponential\n",
    "- Dirichlet\n",
    "\n",
    "Non-examples:\n",
    "- Student t-distribution can't be written in exponential form\n",
    "- Uniform distribution support depends on the parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log-Partition Function\n",
    "\n",
    "Derivatives of the **log-partition function** $A(\\theta)$ yield **cumulants** of the sufficient statistics *(Exercise!)*\n",
    "- $\\nabla_\\theta \\log p(x|\\theta) = E[\\phi(x)]$\n",
    "- $\\nabla^2_\\theta \\log p(x|\\theta) = Cov[ \\phi(x) ]$\n",
    "\n",
    "This guarantees that $A(\\theta)$ is convex!\n",
    "- Its Hessian is the covariance matrix of $X$, which is positive-definite.\n",
    "- Later, this will guarantee a unique global maximum of the likelihood! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### <span style=\"color:gray\">Proof of Convexity: First Derivative</span>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dA}{d\\theta}\n",
    "&= \\frac{d}{d\\theta} \\left[ \\log \\int exp(\\theta\\phi(x))h(x)dx \\right] \\\\\n",
    "&= \\frac{\\frac{d}{d\\theta} \\int exp(\\theta\\phi(x))h(x)dx)}{\\int exp(\\theta\\phi(x))h(x)dx)} \\\\\n",
    "&= \\frac{\\int \\phi(x)exp(\\theta\\phi(x))h(x)dx}{exp(A(\\theta)} \\\\\n",
    "&= \\int \\phi(x) \\exp[\\theta\\phi(x)-A(\\theta)] h(x) dx \\\\\n",
    "&= \\int \\phi(x) p(x) dx \\\\\n",
    "&= E[\\phi(x)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### <span style=\"color:gray\">Proof of Convexity: Second Derivative</span>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d^2A}{d\\theta^2}\n",
    "& = \\int \\phi(x)\\exp[\\theta \\phi(x) - A(\\theta)] h(x) (\\phi(x) - A'(\\theta)) dx \\\\\n",
    "& = \\int \\phi(x) p(x) (\\phi(x) - A'(\\theta))dx \\\\\n",
    "& = \\int \\phi^2(x) p(X) dx - A'(\\theta) \\int \\phi(x)p(x)dx \\\\\n",
    "& = E[\\phi^2(x)] - E[\\phi(x)]^2  \\hspace{2em}   (\\because A'(\\theta) = E[\\phi(x)])  \\\\ \n",
    "& = Var[\\phi(x)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### <span style=\"color:gray\">Proof of Convexity: Second Derivative</span>\n",
    "\n",
    "For multi-variate case, we have \n",
    "\n",
    "$$ \\frac{\\partial^2A}{\\partial\\theta_i \\partial\\theta_j} = E[\\phi_i(x)\\phi_j(x)] - E[\\phi_i(x)] E[\\phi_j(x)]$$\n",
    "\n",
    "and hence,\n",
    "$$ \\nabla^2A(\\theta) = Cov[\\phi(x)] $$\n",
    "\n",
    "Since covariance is positive definite, we have $A(\\theta)$ convex as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  Likelihood\n",
    "\n",
    "For data $\\D = (x_1, \\dots, x_N)$, the likelihood is\n",
    "$$\n",
    "p(\\D|\\theta)\n",
    "= \\left[ \\prod_{k=1}^N h(x_k) \\right] Z(\\theta)^{-N} \\exp\\left[ \\eta(\\theta)^T \\sum_{k=1}^N \\phi(x_k) \\right]\n",
    "$$\n",
    "\n",
    "The sufficient statistics are now $N$ and $\\phi(\\D) = \\sum_{k=1}^N \\phi(x)$.\n",
    "- **Bernoulli:** $N$ and $\\phi = \\# Heads$\n",
    "- **Normal:** $N$ and $\\phi = [ \\sum_k x_k, \\sum_k x_k^2 ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pitman-Koopman-Darmois Theorem\n",
    "\n",
    "> Among families of distributions $P(x | \\theta)$ whose support does not vary with the parameter $\\theta$, only in exponential families is there a sufficient statistic $T(x_1,\\dots,x_N)$ whose dimension remains bounded as the sample size $N$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  MLE\n",
    "\n",
    "For natural parameters $\\theta$ and data $\\D = (x_1, \\dots, x_N)$, \n",
    "$$\n",
    "\\log p(\\D|\\theta) = \\eta^T \\phi(\\D) - N A(\\theta)\n",
    "$$\n",
    "\n",
    "Since $-A(\\theta)$ is concave and $\\theta^T\\phi(\\D)$ linear,\n",
    "- the log-likelihood is concave\n",
    "- there is a unique global maximum!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  MLE\n",
    "\n",
    "To find the maximum, recall $\\nabla_\\theta \\log p(x|\\theta) = E[\\phi(x)]$, so\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\nabla_\\theta \\log p(\\D | \\theta) = \\phi(\\D) - N E[\\phi(X)] = 0 \\\\\n",
    "\\implies E[\\phi(X)] = \\frac{\\phi(\\D)}{N} = \\frac{1}{N} \\sum_{k=1}^N \\phi(x_k)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "At the MLE $\\hat\\theta_{MLE}$, the empirical average of sufficient statistics equals their expected value.\n",
    "- this is called **moment matching**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  MLE\n",
    "\n",
    "As an example, consider the Bernoulli distribution\n",
    "- Sufficient statistic $N$, $\\phi(\\D) = \\# Heads$\n",
    "\n",
    "$$\n",
    "\\hat\\mu_{MLE} = \\frac{\\# Heads}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes for Exponential Family\n",
    "\n",
    "Exact Bayesian analysis is considerably simplified if the prior is **conjugate** to the likelihood.\n",
    "- Simply, this means that prior $p(\\mathcal{D}|\\tau)$ has the same form as likelihood $p(\\mathcal{D}|\\theta)$.\n",
    "\n",
    "This requires likelihood to have finite sufficient statistics\n",
    "* Exponential family to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood\n",
    "\n",
    "Likelihood: \n",
    "$$ p(\\mathcal{D}|\\theta) \\propto g(\\theta)^N \\exp[\\eta(\\theta)^T s_N]\\\\\n",
    "s_N = \\sum_{i=1}^{N}s(x_i)$$\n",
    "\n",
    "In terms of canonical parameters:\n",
    "$$ p(\\mathcal{D}|\\eta) \\propto \\exp[N\\eta^T \\bar{s} -N A(\\eta)] \\\\\n",
    "\\bar s = \\frac{1}{N}s_N $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior\n",
    "\n",
    "$$ p(\\theta| \\nu_0, \\tau_0) \\propto g(\\theta)^{\\nu_0} \\exp[\\eta(\\theta)^T \\tau_0] $$\n",
    "\n",
    "* Denote $ \\tau_0 = \\nu_0 \\bar{\\tau}_0$ to separate out the size of the **prior pseudo-data**, $\\nu_0$ , from the mean of the sufficient statistics on this pseudo-data, $\\tau_0$ . Hence,\n",
    "\n",
    "$$ p(\\theta| \\nu_0, \\bar \\tau_0) \\propto \\exp[\\nu_0\\eta^T \\bar \\tau_0  - \\nu_0 A(\\eta)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior:  Example\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\theta| \\nu_0, \\tau_0) \n",
    "&\\propto (1-\\theta)^{\\nu_0} \\exp[\\tau_0\\log(\\frac{\\theta}{1-\\theta})] \\\\\n",
    "&= \\theta^{\\tau_0}(1-\\theta)^{\\nu_0 - \\tau_0}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Define $\\alpha = \\tau_0 +1 $ and $\\beta = \\nu_0 - \\tau_0 +1$ to see that this is a **beta distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Posterior\n",
    "\n",
    "Posterior: \n",
    "$$ p(\\theta|\\mathcal{D}) = p(\\theta|\\nu_N, \\tau_N) = p(\\theta| \\nu_0 +N, \\tau_0 +s_N) $$\n",
    "\n",
    "Note that we obtain **hyper-parameters** by adding. Hence,\n",
    "\n",
    "$$ \\begin{align}\n",
    "p(\\eta|\\mathcal{D})\n",
    "&\\propto \\exp[\\eta^T (\\nu_0 \\bar\\tau_0 + N \\bar s) - (\\nu_0 + N) A(\\eta) ] \\\\\n",
    "&= p(\\eta|\\nu_0 + N, \\frac{\\nu_0 \\bar\\tau_0 + N \\bar s}{\\nu_0 + N})\n",
    "\\end{align}$$\n",
    "\n",
    "* *posterior hyper-parameters are a convex combination of the prior mean hyper-parameters and the average of the sufficient statistics.*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
